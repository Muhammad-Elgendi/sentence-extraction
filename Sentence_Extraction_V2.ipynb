{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Extraction V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbLGQdvH-WAJ",
        "colab_type": "code",
        "outputId": "ceadab4a-6a85-4172-e0b7-60f413e3dd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from pprint import pprint\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n",
        "nltk.download('punkt')\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    \"\"\"\n",
        "    Translates multiple whitespace into single space character.\n",
        "    If there is at least one new line character chunk is replaced\n",
        "    by single LF (Unix new line) character.\n",
        "    \"\"\"\n",
        "    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n",
        "\n",
        "\n",
        "def _replace_whitespace(match):\n",
        "    text = match.group()\n",
        "\n",
        "    if \"\\n\" in text or \"\\r\" in text:\n",
        "        return \"\\n\"\n",
        "    else:\n",
        "        return \" \"\n",
        "\n",
        "\n",
        "def is_blank(string):\n",
        "    \"\"\"\n",
        "    Returns `True` if string contains only white-space characters\n",
        "    or is empty. Otherwise `False` is returned.\n",
        "    \"\"\"\n",
        "    return not string or string.isspace()\n",
        "\n",
        "\n",
        "def get_symmetric_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Get Symmetric matrix\n",
        "    :param matrix:\n",
        "    :return: matrix\n",
        "    \"\"\"\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "\n",
        "def core_cosine_similarity(vector1, vector2):\n",
        "    \"\"\"\n",
        "    measure cosine similarity between two vectors\n",
        "    :param vector1:\n",
        "    :param vector2:\n",
        "    :return: 0 < cosine similarity value < 1\n",
        "    \"\"\"\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "\n",
        "'''\n",
        "Note: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n",
        "'''\n",
        "\n",
        "\n",
        "class TextRank4Sentences():\n",
        "    def __init__(self):\n",
        "        self.damping = 0.85  # damping coefficient, usually is .85\n",
        "        self.min_diff = 1e-5  # convergence threshold\n",
        "        self.steps = 100  # iteration steps\n",
        "        self.text_str = None\n",
        "        self.sentences = None\n",
        "        self.pr_vector = None\n",
        "\n",
        "    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n",
        "        if stopwords is None:\n",
        "            stopwords = []\n",
        "\n",
        "        sent1 = [w.lower() for w in sent1]\n",
        "        sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "        all_words = list(set(sent1 + sent2))\n",
        "\n",
        "        vector1 = [0] * len(all_words)\n",
        "        vector2 = [0] * len(all_words)\n",
        "\n",
        "        # build the vector for the first sentence\n",
        "        for w in sent1:\n",
        "            if w in stopwords:\n",
        "                continue\n",
        "            vector1[all_words.index(w)] += 1\n",
        "\n",
        "        # build the vector for the second sentence\n",
        "        for w in sent2:\n",
        "            if w in stopwords:\n",
        "                continue\n",
        "            vector2[all_words.index(w)] += 1\n",
        "\n",
        "        return core_cosine_similarity(vector1, vector2)\n",
        "\n",
        "    def _build_similarity_matrix(self, sentences, stopwords=None):\n",
        "        # create an empty similarity matrix\n",
        "        sm = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1 == idx2:\n",
        "                    continue\n",
        "\n",
        "                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n",
        "\n",
        "        # Get Symmeric matrix\n",
        "        sm = get_symmetric_matrix(sm)\n",
        "\n",
        "        # Normalize matrix by column\n",
        "        norm = np.sum(sm, axis=0)\n",
        "        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n",
        "\n",
        "        return sm_norm\n",
        "\n",
        "    def _run_page_rank(self, similarity_matrix):\n",
        "\n",
        "        pr_vector = np.array([1] * len(similarity_matrix))\n",
        "\n",
        "        # Iteration\n",
        "        previous_pr = 0\n",
        "        for epoch in range(self.steps):\n",
        "            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n",
        "            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n",
        "                break\n",
        "            else:\n",
        "                previous_pr = sum(pr_vector)\n",
        "\n",
        "        return pr_vector\n",
        "\n",
        "    def _get_sentence(self, index):\n",
        "\n",
        "        try:\n",
        "            return self.sentences[index]\n",
        "        except IndexError:\n",
        "            return \"\"\n",
        "\n",
        "    def get_top_sentences(self, number=5):\n",
        "\n",
        "        sorted_sent = []\n",
        "        if self.pr_vector is not None:\n",
        "\n",
        "            sorted_pr = np.argsort(self.pr_vector)\n",
        "            sorted_pr = list(sorted_pr)\n",
        "            sorted_pr.reverse()\n",
        "\n",
        "            index = 0\n",
        "            for epoch in range(number):\n",
        "                # print (str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n",
        "                sent = self.sentences[sorted_pr[index]]\n",
        "                sent = normalize_whitespace(sent)\n",
        "                sorted_sent.append([sent,self.pr_vector[sorted_pr[index]]])\n",
        "                index += 1\n",
        "\n",
        "        return sorted_sent\n",
        "\n",
        "    def analyze(self, text, stop_words=None):\n",
        "        self.text_str = text\n",
        "        self.sentences = sent_tokenize(self.text_str)\n",
        "\n",
        "        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n",
        "\n",
        "        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n",
        "\n",
        "        self.pr_vector = self._run_page_rank(similarity_matrix)\n",
        "        # print(self.pr_vector)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Buky4NYWv8PV",
        "colab_type": "code",
        "outputId": "761881fb-a0d0-4106-95d4-1338e8774462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "# read summarized results from file\n",
        "corpusFile=open('summaried-results.txt','r', encoding='utf-8')\n",
        "corpus = corpusFile.read()\n",
        "\n",
        "# converts to lowercase\n",
        "corpus = corpus.lower()\n",
        "\n",
        "sentenceRanker = TextRank4Sentences()\n",
        "sentenceRanker.analyze(corpus)\n",
        "top_sentences = sentenceRanker.get_top_sentences(10)\n",
        "\n",
        "pprint(top_sentences)\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['semantic seo is a marketing technique to improve the traffic of a website '\n",
            "  'by providing search engines with metadata and semantically relevant content '\n",
            "  'that can unambiguously answer a specific search intent .',\n",
            "  1.4620482973797637],\n",
            " ['the value here is obvious : the post can afford to lose keyword rankings '\n",
            "  'for one of these search queries and still sustain a good amount of its '\n",
            "  'organic traffic in the long run .',\n",
            "  1.4002656266837787],\n",
            " ['the serp landscape is a long way from ten blue links ; video and image '\n",
            "  'thumbnails , authorship thumbnails , and rich snippets of many types now '\n",
            "  'fundamentally alter what users click on : it will be interesting to see '\n",
            "  'what testing data and correlation studies tell us about structured data '\n",
            "  'markup as a ranking factor.',\n",
            "  1.341909442649909],\n",
            " ['there was a focus on semantic search and structured data markup at the show '\n",
            "  ', reflecting the expansion of schema.org and google knowledge graph as well '\n",
            "  'as bing snapshots and the growing influence of the open graph protocol .',\n",
            "  1.3413401082568925],\n",
            " [\"however , there 's still not an agreed-upon term to describe the activities \"\n",
            "  'around achieving visibility in the semantic search results or optimizing '\n",
            "  'for a semantic search engine .',\n",
            "  1.3055290349570492],\n",
            " ['conclusion offering value , building relevancy , and thinking about the new '\n",
            "  'problems your customers will face is what will separate your post from all '\n",
            "  'the other ones over a longer period of time .',\n",
            "  1.2518375284098888],\n",
            " ['in 2011 as google and other search engines began moving towards artificial '\n",
            "  'intelligence and natural language processing to understand the searcher â€™ s '\n",
            "  'intent and the meaning of a query they started to work with entities and '\n",
            "  'concepts rather than parsing questions and web pages using keywords .',\n",
            "  1.250246203449672],\n",
            " ['when you click on the blue links within the field of related searches , '\n",
            "  'they will generate a completely different search results page .',\n",
            "  1.2500608457034974],\n",
            " [\"justin briggs wrote a piece about entity search results that 's over a year \"\n",
            "  \"old , and it 's still a useful primer on how search engines are \"\n",
            "  'increasingly moving towards these kinds of results for user queries .',\n",
            "  1.1840787981538345],\n",
            " ['they want to know : what a pex plumbing pipe is .', 1.1782503075197512]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}